package com.trellocursos.main;

import java.util.Arrays;
import java.util.List;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class Index {

	public static void main(String[] args) {
		String list = "Sección: 1\r\n" + "0 / 4\r\n" + "Introducción\r\n" + "1. Introducción\r\n" + "06:53\r\n"
				+ "2. Pre requisitos del curso\r\n" + "01:37\r\n"
				+ "3. Conoce a tu instructor online, Juan Gabriel Gomila\r\n" + "02:25\r\n"
				+ "4. Acerca de la valoración prematura del curso en Udemy\r\n" + "02:41\r\n" + "Sección: 2\r\n"
				+ "0 / 7\r\n" + "Instalando nuestra herramienta de trabajo\r\n"
				+ "5. Cómo instalar Python con Anaconda Navigator\r\n" + "11:21\r\n"
				+ "Cómo instalar python en cualquier sistema operativo\r\n"
				+ "6. Cómo instalar paquetes en Python gracias a pip\r\n" + "12:01\r\n"
				+ "7. Instalación de pip en Windows\r\n" + "8. Las librerías estándar de Machine Learning en Python\r\n"
				+ "19:03\r\n" + "9. Los editores para programar en Python\r\n" + "09:53\r\n"
				+ "10. Las 5 etapas del análisis de datos\r\n" + "13:10\r\n"
				+ "11. Comunidad de estudiantes del curso\r\n" + "03:37\r\n" + "Comunidad de Estudiantes del Curso\r\n"
				+ "Sección: 3\r\n" + "0 / 4\r\n" + "Una introducción al análisis predictivo y al Machine Learning\r\n"
				+ "12. Ser Data Scientist es la profesión más sexy del siglo XXI\r\n" + "15:46\r\n"
				+ "13. ¿Qué es el análisis predictivo de datos?\r\n" + "18:49\r\n"
				+ "14. Data Scientist = Matemáticas + Programación + Business\r\n" + "10:48\r\n"
				+ "15. Aplicaciones y ejemplos del mundo del Data Science\r\n" + "26:31\r\n" + "Sección: 4\r\n"
				+ "0 / 19\r\n" + "Limpieza de Datos\r\n" + "16. Data Cleaning\r\n" + "04:40\r\n"
				+ "17. El concepto de data frame\r\n" + "09:23\r\n" + "18. El repositorio Git del curso\r\n"
				+ "05:28\r\n" + "El repositorio Git del Curso\r\n" + "19. Leer datos procedentes de un CSV\r\n"
				+ "13:50\r\n" + "20. Los parámetros de la función read_csv\r\n" + "17:26\r\n"
				+ "21. Ejemplos de diferentes carga de datos con read_csv\r\n" + "14:58\r\n"
				+ "22. El método open para la carga manual de datos\r\n" + "14:39\r\n"
				+ "23. Cuidado con el método open\r\n" + "24. Leer y escribir en un fichero con Python\r\n"
				+ "07:56\r\n" + "25. Leer los datos desde una URL externa\r\n" + "12:45\r\n"
				+ "26. La carga de datos desde una hoja de cálculo\r\n" + "08:35\r\n"
				+ "27. Ejercicio: descargar y procesar datos desde una URL externa\r\n" + "19:24\r\n"
				+ "28. Las funciones básicas de resumen, estructura, dimensiones y cabecera\r\n" + "16:45\r\n"
				+ "29. ¿Por qué faltan valores en los data sets?\r\n" + "13:53\r\n"
				+ "30. Qué hacer cuando faltan valores en el dataset\r\n" + "18:24\r\n" + "31. Las variables dummy\r\n"
				+ "13:18\r\n" + "32. Visualización básica de un dataset: el scatterplot\r\n" + "15:35\r\n"
				+ "33. Visualización básica de un dataset: el histograma de frecuencias\r\n" + "09:45\r\n"
				+ "34. Visualización básica de un dataset: el boxplot\r\n" + "12:58\r\n" + "Sección: 5\r\n"
				+ "0 / 30\r\n" + "Operaciones de manejo de datos\r\n" + "35. Data Wrangling\r\n" + "07:29\r\n"
				+ "36. Una chuleta de pandas para Data Wrangling\r\n"
				+ "2018-05-09_23-40-38-41dcf24f24c7e29a782f135742c028c6.jpg\r\n"
				+ "2018-05-09_23-40-38-fe3ca92ce37648006c5d5e94ed873b61.jpg\r\n" + "37. Fe de erratas\r\n"
				+ "38. Buscar un subconjunto de datos de un dataset\r\n" + "19:54\r\n"
				+ "39. Filtrados alternativos\r\n" + "40. Subconjuntos de filas con ciertas condiciones\r\n"
				+ "18:12\r\n" + "41. Subconjuntos con loc e iloc y creación de nuevas columnas\r\n" + "16:33\r\n"
				+ "42. Generar números aleatorios\r\n" + "19:45\r\n" + "43. La semilla de la generación aleatoria\r\n"
				+ "05:06\r\n" + "44. Funciones de distribución de probabilidades\r\n" + "05:41\r\n"
				+ "45. La distribución uniforme\r\n" + "08:59\r\n" + "46. La distribución Normal\r\n" + "15:10\r\n"
				+ "47. El método de la simulación de Monte-Carlo para encontrar el valor de Pi\r\n" + "21:56\r\n"
				+ "48. Generando dummy data frames\r\n" + "10:51\r\n"
				+ "49. Un dummy data frame con variables categóricas\r\n" + "12:05\r\n"
				+ "50. Agrupación de los datos por categorías\r\n" + "08:13\r\n" + "51. Agregación de datos\r\n"
				+ "11:22\r\n" + "52. Filtrado, Transformación y otras operaciones útiles\r\n" + "14:40\r\n"
				+ "53. Conjunto de entrenamiento y de testing\r\n" + "05:48\r\n"
				+ "54. Atualización: cómo dividir conjunto de entrenamiento y test\r\n"
				+ "55. Muestreo aleatorio: cómo dividir un dataset en conjunto de entreno y validación\r\n"
				+ "13:30\r\n" + "56. Concatenar dos datasets por filas\r\n" + "17:18\r\n"
				+ "57. Carga de cientos de datos distribuidos\r\n" + "23:18\r\n"
				+ "58. Ejercicio: el data set de los juegos olímpicos\r\n" + "19:57\r\n"
				+ "59. Concatenar los datos con merge\r\n" + "13:46\r\n" + "60. Formas de cruzar tablas con joins\r\n"
				+ "14:26\r\n" + "61. Eliminar datos de datasets con restricciones de conjunto\r\n" + "13:01\r\n"
				+ "62. Ejemplos de joins con Python\r\n" + "16:41\r\n"
				+ "63. Ya conoces las bases del manejo de datos\r\n" + "04:00\r\n"
				+ "64. ¿Te gusta el curso? ¡Valóralo y cuéntanos tu opinión!\r\n" + "01:44\r\n" + "Sección: 6\r\n"
				+ "0 / 8\r\n" + "Conceptos básicos de estadística para la modelización predictiva\r\n"
				+ "65. Los conceptos fundamentales de estadística\r\n" + "06:59\r\n"
				+ "66. Un resumen de los estadísticos básicos (en R)\r\n" + "25:46\r\n"
				+ "67. Muestreo aleatorio y el teorema central del límite\r\n" + "12:26\r\n"
				+ "68. Test de la chi cuadrado\r\n" + "12:46\r\n" + "69. Los contrastes de hipótesis\r\n" + "19:02\r\n"
				+ "70. Cómo hacer un contaste de hipótesis paso a paso\r\n" + "16:19\r\n"
				+ "71. Correlación entre variables\r\n" + "30:38\r\n" + "72. Un resumen de lo aprendido\r\n"
				+ "03:24\r\n" + "Sección: 7\r\n" + "0 / 22\r\n" + "Regresión lineal con Python\r\n"
				+ "73. La regresión lineal\r\n" + "06:24\r\n" + "74. Las matemáticas tras una regresión lineal\r\n"
				+ "10:18\r\n" + "75. Demostración de la obtención de los parámetros del modelo lineal\r\n" + "19:52\r\n"
				+ "76. Errores normalmente distribuidos\r\n" + "02:24\r\n"
				+ "77. Sumas de los cuadrados totales, de las diferencias y de la regresión\r\n" + "32:22\r\n"
				+ "78. Ejercicio demostrar que SST = SSR + SSD\r\n"
				+ "79. Encontrando los coeficientes óptimos de la regresión\r\n" + "14:42\r\n"
				+ "80. Interpretar los parámetros de la regresión\r\n" + "14:51\r\n"
				+ "81. Implementar una regresión lineal con Python\r\n" + "20:49\r\n"
				+ "82. Regresión lineal múltiple\r\n" + "23:13\r\n" + "83. El problema de la multicolinealidad\r\n"
				+ "12:13\r\n" + "84. Validando nuestro modelo\r\n" + "14:54\r\n"
				+ "85. El resumen de todos los modelos lineales creados\r\n" + "06:11\r\n"
				+ "86. Regresión lineal con scikit-learn\r\n" + "11:46\r\n"
				+ "87. Modelos lineales con variables categóricas\r\n" + "06:13\r\n"
				+ "88. Variables categóricas en una regresión lineal\r\n" + "27:43\r\n"
				+ "89. Otra forma más simple de calcular las predicciones\r\n"
				+ "90. Enmascarado de variables categóricas redundantes\r\n" + "16:05\r\n"
				+ "91. Transformar las variables en relaciones no lineales\r\n" + "33:22\r\n"
				+ "92. El problema de los outliers\r\n" + "16:38\r\n"
				+ "93. Otros problemas y consideraciones de la regresión lineal\r\n" + "11:17\r\n"
				+ "94. Un resumen de la regresión lineal\r\n" + "05:49\r\n" + "Sección: 8\r\n" + "0 / 17\r\n"
				+ "Regresión logística con Python\r\n" + "95. La regresión logística\r\n" + "03:21\r\n"
				+ "96. Regresión lineal vs regresión logística\r\n" + "07:26\r\n"
				+ "97. Las matemáticas detrás de la regresión logística\r\n" + "11:01\r\n"
				+ "98. Probabilidades condicionadas\r\n" + "13:03\r\n" + "99. Cociente de probabilidades\r\n"
				+ "13:41\r\n" + "100. De la regresión lineal a la logística\r\n" + "19:25\r\n"
				+ "101. Estimación con el método de máxima verosimilitud\r\n" + "40:10\r\n"
				+ "102. Crear un modelo logístico desde cero\r\n" + "49:56\r\n"
				+ "103. Análisis exploratorio de los datos\r\n" + "29:33\r\n"
				+ "104. La selección de variables del dataset para el modelo logístico\r\n" + "21:10\r\n"
				+ "105. Implementar una regresión logística con Python\r\n" + "13:50\r\n"
				+ "106. Validación del modelo y evaluación del mismo\r\n" + "16:12\r\n"
				+ "107. La validación cruzada\r\n" + "13:39\r\n" + "108. Validación cruzada con Python\r\n"
				+ "11:56\r\n" + "109. Las matrices de confusión y las curvas ROC\r\n" + "08:14\r\n"
				+ "110. Implementación de las curvas ROC en Python\r\n" + "31:30\r\n"
				+ "111. Resumen de la regresión logística\r\n" + "06:19\r\n" + "Sección: 9\r\n" + "0 / 22\r\n"
				+ "Clustering y clasificación\r\n" + "112. Clustering\r\n" + "06:16\r\n"
				+ "113. ¿Qué es y para qué sirve el clustering?\r\n" + "17:35\r\n" + "114. El concepto de distancia\r\n"
				+ "16:23\r\n" + "115. Matriz de distancias en Python\r\n" + "12:16\r\n" + "116. Métodos de enlace\r\n"
				+ "11:41\r\n" + "117. Uniendo datos manualmente\r\n" + "26:34\r\n"
				+ "118. Clustering jerárquico en Python\r\n" + "14:03\r\n"
				+ "119. Un clustering completo: la fase de exploración de datos\r\n" + "17:58\r\n"
				+ "120. Un clustering completo: representación del dendrograma\r\n" + "29:04\r\n"
				+ "121. Un clustering completo: por donde cortamos el dendrograma\r\n" + "30:18\r\n"
				+ "122. Un clustering completo: visualización final del clustering\r\n" + "10:14\r\n"
				+ "123. El método de k-means\r\n" + "07:08\r\n" + "124. Implementando k-means con Python\r\n"
				+ "08:52\r\n" + "125. Ejercicio: Segmentación de los vinos\r\n" + "27:27\r\n"
				+ "126. El método del codo\r\n" + "03:51\r\n" + "127. El coeficiente de la silueta\r\n" + "07:03\r\n"
				+ "128. Implementando la técnica del codo y el coeficiente de la silueta\r\n" + "35:08\r\n"
				+ "129. Propagación de la afinidad\r\n" + "07:57\r\n"
				+ "130. Implementando la propagación de la afinidad\r\n" + "19:06\r\n"
				+ "131. Generando distribuciones en forma de anillo\r\n" + "10:54\r\n"
				+ "132. Los K medoides y el clustering espectral\r\n" + "15:39\r\n" + "133. Resumen del clustering\r\n"
				+ "03:58\r\n" + "Sección: 10\r\n" + "0 / 18\r\n" + "Árboles y bosques aleatorios\r\n"
				+ "134. Árboles y bosques aleatorios\r\n" + "03:35\r\n" + "135. ¿Qué es un árbol de decisión?\r\n"
				+ "09:16\r\n" + "136. Homogeneidad en los datos\r\n" + "07:11\r\n"
				+ "137. Entropía y ganancia de Información\r\n" + "15:22\r\n"
				+ "138. Algoritmos para la generación de árboles de clasificación\r\n" + "15:53\r\n"
				+ "139. La poda del árbol\r\n" + "06:45\r\n" + "140. Los problemas del árbol\r\n" + "12:13\r\n"
				+ "141. Los árboles de clasificación con Python\r\n" + "16:23\r\n"
				+ "142. El tratamiento de ficheros dot\r\n" + "15:27\r\n"
				+ "143. La validación cruzada en un árbol de clasificación\r\n" + "12:29\r\n"
				+ "144. Los árboles de regresión\r\n" + "08:07\r\n"
				+ "145. El dataset de las casas de Boston y Kaggle\r\n" + "10:42\r\n"
				+ "146. Árboles de regresión con Python\r\n" + "16:56\r\n" + "147. Random forests\r\n" + "09:34\r\n"
				+ "148. Random forests para regresión\r\n" + "09:57\r\n" + "149. Random forest para clasificación\r\n"
				+ "04:44\r\n" + "150. ¿Por qué funcionan los random forests?\r\n" + "06:31\r\n"
				+ "151. Resumen de árboles y bosques aleatorios\r\n" + "03:35\r\n" + "Sección: 11\r\n" + "0 / 17\r\n"
				+ "Máquinas de Soporte Vectorial\r\n" + "152. Las máquinas de soporte vectorial\r\n" + "04:51\r\n"
				+ "153. Las support vector machines\r\n" + "12:12\r\n"
				+ "154. El problema de clasificación no óptimo\r\n" + "14:02\r\n"
				+ "155. Los núcleos no lineales y el problema de la dimensión\r\n" + "10:59\r\n"
				+ "156. Soporte Vectorial Clasificador Lineal\r\n" + "07:40\r\n"
				+ "157. Creando el modelo clasificador lineal\r\n" + "08:08\r\n"
				+ "158. Representación gráfica del hiperplano separador en 2D\r\n" + "07:25\r\n"
				+ "159. El problema de la separación\r\n" + "09:36\r\n"
				+ "160. Maximizar el margen de clasificación\r\n" + "04:58\r\n" + "161. Los soportes de SVM\r\n"
				+ "26:21\r\n" + "162. Kernels no lineales\r\n" + "13:26\r\n" + "163. Radial basis function\r\n"
				+ "09:32\r\n" + "164. Ajustando las SVM\r\n" + "13:29\r\n"
				+ "165. Práctica de SVM: reconocimiento facial a lo CSI\r\n" + "31:44\r\n"
				+ "166. Práctica de SVM: Clasificación de las flores de Iris\r\n" + "28:35\r\n"
				+ "167. Truco: qué hacer cuando me toca hacer una análisis de datos\r\n" + "13:37\r\n"
				+ "168. SVM para regresión\r\n" + "17:48\r\n" + "Sección: 12\r\n" + "0 / 9\r\n"
				+ "K Nearest Neighbors\r\n" + "169. La decisión de los K vecinos\r\n" + "02:54\r\n"
				+ "170. Los k vecinos más cercanos\r\n" + "11:26\r\n" + "171. Limpieza del dataset del Cancer\r\n"
				+ "08:29\r\n" + "172. Clasificación según los K vecinos\r\n" + "09:43\r\n"
				+ "173. Clasificando nuevos datos de los tests médicos\r\n" + "06:55\r\n"
				+ "174. Creando los datos para la clasificación\r\n" + "10:36\r\n"
				+ "175. Implementando la decisión por mayoría\r\n" + "16:09\r\n"
				+ "176. Nuestro algoritmo vs scikit-learn\r\n" + "13:04\r\n"
				+ "177. Una opinión final sobre los algoritmos de Machine Learning\r\n" + "06:38\r\n"
				+ "Sección: 13\r\n" + "0 / 12\r\n" + "Sistemas de recomendación\r\n"
				+ "178. El rol de las recomendaciones dinámicas en el siglo XXI\r\n" + "05:14\r\n"
				+ "179. El dataset de películas de Movie Lens\r\n" + "07:18\r\n"
				+ "180. Análisis exploratorio de los datos y distribución de las valoraciones\r\n" + "06:27\r\n"
				+ "181. Esparseidad de los datos\r\n" + "06:31\r\n" + "182. División en entrenamiento y validación\r\n"
				+ "04:03\r\n" + "183. La matriz de similaridad entre usuarios\r\n" + "07:28\r\n"
				+ "184. Predecir la valoración de un ítem para un usuario\r\n" + "06:16\r\n"
				+ "185. Corrección: Error en la clase anterior\r\n" + "186. Filtrando con los K nearest neighbors\r\n"
				+ "13:34\r\n" + "187. Sistemas de Recomendación basados en Ítems\r\n" + "10:47\r\n"
				+ "188. Recomendando con los K items más parecidos\r\n" + "06:27\r\n"
				+ "189. Los resultados finales\r\n" + "08:31\r\n" + "Sección: 14\r\n" + "0 / 13\r\n"
				+ "Análisis de componentes principales\r\n" + "190. Análisis de Componentes principales\r\n"
				+ "03:57\r\n" + "191. El problema de la dimensión\r\n" + "10:31\r\n"
				+ "192. Demostración de cómo se hace un ACP\r\n" + "28:12\r\n"
				+ "193. Implementando nuestro propio ACP en Python\r\n" + "08:53\r\n"
				+ "194. Plotly, la librería de gráficos personalizados e interactivos\r\n" + "26:26\r\n"
				+ "195. Los valores y vectores propios de la matriz de covarianzas\r\n" + "10:21\r\n"
				+ "196. La matriz de correlaciones y el Singular Value Decomposition\r\n" + "06:22\r\n"
				+ "197. La selección de las componentes principales\r\n" + "18:17\r\n"
				+ "198. La proyección en el subespacio vectorial resultante\r\n" + "10:10\r\n"
				+ "199. Implementación de ACP con sklearn\r\n" + "08:00\r\n" + "200. Más gráficos con Plotly\r\n"
				+ "09:37\r\n" + "201. Personalizando los gráficos de plotly\r\n" + "17:25\r\n"
				+ "202. Coloraciones y etiquetas de plotly\r\n" + "15:59\r\n" + "Sección: 15\r\n" + "0 / 17\r\n"
				+ "Introducción a las redes neuronales y al deep learning con TensorFlow\r\n"
				+ "203. Redes neuronales del futuro\r\n" + "06:42\r\n" + "204. Introducción a Tensor Flow\r\n"
				+ "05:22\r\n" + "205. Acerca de las redes neuronales y el deep learning\r\n" + "11:52\r\n"
				+ "206. Instalando TensorFlow en tu ordenador\r\n" + "12:35\r\n"
				+ "207. La carga del dataset de imágenes\r\n" + "22:59\r\n"
				+ "208. Análisis exploratorio de los datos\r\n" + "11:17\r\n" + "209. Un resumen visual de imágenes\r\n"
				+ "16:59\r\n" + "210. Pre procesado de imágenes previo al ML\r\n" + "15:06\r\n"
				+ "211. Creación del modelo\r\n" + "14:13\r\n" + "212. Entrenamiento del modelo\r\n" + "10:22\r\n"
				+ "213. Validación del modelo\r\n" + "21:18\r\n" + "214. El dataset de reconocimiento de dígitos\r\n"
				+ "08:08\r\n" + "215. De datos desestructurados a espacios vectoriales n-dimensionales\r\n"
				+ "05:24\r\n" + "216. La regresión softmax\r\n" + "09:31\r\n"
				+ "217. Tensorflow y la regresión softmax\r\n" + "10:53\r\n"
				+ "218. La fase de entrenamiento de la red neuronal\r\n" + "15:45\r\n"
				+ "219. La fase de evaluación de la red neuronal\r\n" + "06:43\r\n" + "Sección: 16\r\n" + "0 / 9\r\n"
				+ "Juntar código de R y Python con la librería rpy2\r\n"
				+ "220. Cuando Python conoce a R, no hay límites en el Big Data\r\n" + "03:10\r\n"
				+ "221. Instalar la librería rpy2\r\n" + "09:49\r\n"
				+ "222. Nota adicional para instalar rpy2 en Windows\r\n" + "223. Llevando objetos de R a Python\r\n"
				+ "07:29\r\n" + "224. Llevando objetos de Python a R\r\n" + "10:11\r\n"
				+ "225. Cómo instalar y cargar paquetes de R desde Python\r\n" + "05:49\r\n"
				+ "226. La librería extRemes en acción desde Python\r\n" + "22:00\r\n" + "227. Rmagic\r\n" + "11:34\r\n"
				+ "228. Lo bueno de programación en Python, lo mejor de estadística con R\r\n" + "13:52\r\n"
				+ "Sección: 17\r\n" + "0 / 3\r\n" + "¿Qué nos depara el futuro?\r\n"
				+ "229. Proyecto final. ¿Qué me depara el futuro?\r\n" + "09:31\r\n"
				+ "230. Nos vemos en el próximo curso\r\n" + "01:44\r\n" + "231. Un regalo para ti";

		list.replaceAll("\\r", "");
		List<String> arrList = Arrays.asList(list.split("\\n"));
//		List<String> arrClean = new ArrayList<String>();
		Pattern pattern = Pattern.compile("[0-9]+[.][ ][a-zA-Z]*");
		for (int i = 0; i < arrList.size(); i++) {
			Matcher matcher = pattern.matcher(arrList.get(i));
			while (matcher.find()) {
//				arrClean.add(s);
				System.out.print(arrList.get(i));
			}
			if (arrList.get(i).startsWith("Quiz")) {
				System.out.print(arrList.get(i));
			}
			if (arrList.get(i).startsWith("Sección")) {
				System.out.println("\n\n");
				System.out.print(arrList.get(i).replaceAll("\r", "") + " - " + arrList.get(i + 2));
				i += 2;
			}
		}

//		for (String s : arrClean)
//			System.out.print(s);

	}

}
